{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langdetect==1.0.9 pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Message Parsing and Normalization\n",
        "\n",
        "This notebook handles the extraction, cleaning, and normalization of messages from multiple platforms (WhatsApp, Telegram).\n",
        "\n",
        "## Overview\n",
        "- **Extract messages** from different export formats (JSON for Telegram, TXT for WhatsApp)\n",
        "- **Clean and normalize** multilingual text while preserving emojis and essential punctuation\n",
        "- **Detect language** and classify conversations (French/English)\n",
        "- **Clean usernames** and filter system messages\n",
        "- **Output standardized CSV** format with timestamp, sender, message columns\n",
        "\n",
        "## Input Data\n",
        "- Raw exports from messaging platforms in `data/raw/` folder\n",
        "- **Telegram**: JSON export files (`data/raw/telegram/result.json`)\n",
        "- **WhatsApp**: TXT export files (`data/raw/whatsapp/*.txt`)\n",
        "\n",
        "## Output Data\n",
        "- **Standardized CSV files** with columns: `timestamp`, `sender`, `message`\n",
        "- One CSV per conversation in `data/cleaned/` folder\n",
        "- **Language detection** per conversation (`conversation_languages.csv`)\n",
        "- **Message statistics** and processing summary\n",
        "\n",
        "## User Configuration\n",
        "- Uses `config/user_config.json` to identify user's messages\n",
        "- Automatically creates default config if not found\n",
        "- Supports multiple user pseudonyms/identifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Telegram parsing functions\n",
        "from typing import Dict, Any, List\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def load_user_config(config_path: str = 'config/user_config.json') -> Dict[str, Any]:\n",
        "    \"\"\"Load user configuration for identifying the user's messages.\"\"\"\n",
        "    if not os.path.exists(config_path):\n",
        "        # Create default config if it doesn't exist\n",
        "        default_config = {\n",
        "            \"user_identifiers\": [\"Antonin\", \"antonin\", \"Anto\", \"anto\"],\n",
        "            \"user_label\": \"A:\",\n",
        "            \"other_label\": \"B:\",\n",
        "            \"description\": \"Configuration for identifying the user's messages in chat data.\"\n",
        "        }\n",
        "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "        with open(config_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(default_config, f, indent=2)\n",
        "        print(f\"Created default user config at {config_path}\")\n",
        "        return default_config\n",
        "    \n",
        "    with open(config_path, 'r', encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded user config: {config['user_identifiers']} -> {config['user_label']}\")\n",
        "    return config\n",
        "\n",
        "def parse_telegram_text(text_data) -> str:\n",
        "    \"\"\"Parse Telegram text which can be a string or list of entities.\"\"\"\n",
        "    if isinstance(text_data, str):\n",
        "        return text_data\n",
        "    elif isinstance(text_data, list):\n",
        "        # Extract text from text_entities\n",
        "        full_text = \"\"\n",
        "        for entity in text_data:\n",
        "            if isinstance(entity, dict):\n",
        "                if entity.get('type') == 'plain' and 'text' in entity:\n",
        "                    full_text += entity['text']\n",
        "                elif 'text' in entity:\n",
        "                    full_text += entity['text']\n",
        "            elif isinstance(entity, str):\n",
        "                full_text += entity\n",
        "        return full_text\n",
        "    else:\n",
        "        return str(text_data) if text_data else \"\"\n",
        "\n",
        "def parse_telegram_message(msg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Parse a single Telegram message into standardized format.\"\"\"\n",
        "    if msg.get('type') != 'message':\n",
        "        return None\n",
        "    \n",
        "    # Parse text content\n",
        "    text = parse_telegram_text(msg.get('text', ''))\n",
        "    if not text or len(text.strip()) < 3:\n",
        "        return None\n",
        "    \n",
        "    # Parse timestamp\n",
        "    try:\n",
        "        if 'date' in msg:\n",
        "            dt = datetime.fromisoformat(msg['date'].replace('Z', '+00:00'))\n",
        "        elif 'date_unixtime' in msg:\n",
        "            dt = datetime.fromtimestamp(int(msg['date_unixtime']))\n",
        "        else:\n",
        "            return None\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "    \n",
        "    return {\n",
        "        'timestamp': dt.isoformat(),\n",
        "        'sender': (msg.get('from') or 'Unknown').strip(),\n",
        "        'message': text.strip()\n",
        "    }\n",
        "\n",
        "def load_telegram_conversation(chat_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load and parse a Telegram chat conversation.\"\"\"\n",
        "    messages = []\n",
        "    \n",
        "    if 'messages' not in chat_data:\n",
        "        return messages\n",
        "    \n",
        "    for msg in chat_data['messages']:\n",
        "        parsed = parse_telegram_message(msg)\n",
        "        if parsed:\n",
        "            messages.append(parsed)\n",
        "    \n",
        "    return messages\n",
        "\n",
        "def load_all_telegram_conversations(file_path: str = 'data/raw/telegram/result.json') -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"Load all Telegram conversations from the export file.\"\"\"\n",
        "    conversations = {}\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Telegram file not found: {file_path}\")\n",
        "        return conversations\n",
        "    \n",
        "    print(f\"Loading Telegram conversations from {file_path}...\")\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        if 'chats' not in data or 'list' not in data['chats']:\n",
        "            print(\"No chats found in Telegram export\")\n",
        "            return conversations\n",
        "        \n",
        "        chats = data['chats']['list']\n",
        "        print(f\"Found {len(chats)} chats in Telegram export\")\n",
        "        \n",
        "        for chat in chats:\n",
        "            chat_name = chat.get('name', f\"chat_{chat.get('id', 'unknown')}\")\n",
        "            chat_type = chat.get('type', 'unknown')\n",
        "            \n",
        "            # Skip system chats and very small conversations\n",
        "            if chat_type in ['personal_chat'] and chat.get('id') == 777000:  # Telegram system\n",
        "                continue\n",
        "            \n",
        "            messages = load_telegram_conversation(chat)\n",
        "            \n",
        "            # Only process conversations with reasonable message count\n",
        "            if len(messages) < 10:\n",
        "                continue\n",
        "            \n",
        "            # Use a clean filename\n",
        "            clean_name = re.sub(r'[^\\w\\s-]', '', chat_name).strip()\n",
        "            clean_name = re.sub(r'[-\\s]+', '_', clean_name)\n",
        "            filename = f\"{clean_name}_telegram.txt\"\n",
        "            \n",
        "            conversations[filename] = messages\n",
        "            print(f\"  Loaded {filename}: {len(messages)} messages\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Telegram conversations: {e}\")\n",
        "    \n",
        "    return conversations\n",
        "\n",
        "def is_french_conversation(messages: List[Dict[str, Any]], sample_size: int = 20) -> bool:\n",
        "    \"\"\"Check if a conversation is in French using langdetect library.\"\"\"\n",
        "    if not messages:\n",
        "        return False\n",
        "    \n",
        "    # Sample messages for detection\n",
        "    sample_messages = messages[:min(sample_size, len(messages))]\n",
        "    sample_texts = []\n",
        "    \n",
        "    for msg in sample_messages:\n",
        "        text = msg.get('message', '').strip()\n",
        "        if len(text) >= 3:\n",
        "            sample_texts.append(text)\n",
        "    \n",
        "    if not sample_texts:\n",
        "        return False\n",
        "    \n",
        "    # Use langdetect to detect language\n",
        "    try:\n",
        "        from langdetect import detect, DetectorFactory\n",
        "        DetectorFactory.seed = 0  # For consistent results\n",
        "        \n",
        "        # Detect language of each sample text\n",
        "        languages = []\n",
        "        for text in sample_texts:\n",
        "            try:\n",
        "                lang = detect(text)\n",
        "                languages.append(lang)\n",
        "            except:\n",
        "                continue  # Skip texts that can't be detected\n",
        "        \n",
        "        if not languages:\n",
        "            return False\n",
        "        \n",
        "        # Count French detections\n",
        "        french_count = languages.count('fr')\n",
        "        french_ratio = french_count / len(languages)\n",
        "        \n",
        "        # Consider French if at least 60% of messages are detected as French\n",
        "        is_french = french_ratio >= 0.6\n",
        "        \n",
        "        print(f\"    Languages detected: {languages[:10]}... (showing first 10)\")\n",
        "        print(f\"    French ratio: {french_ratio:.2f}, Result: {is_french}\")\n",
        "        \n",
        "        return is_french\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"    Error in language detection: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Telegram conversations and filter for French\n",
        "print(\"=== LOADING TELEGRAM CONVERSATIONS ===\")\n",
        "telegram_conversations = load_all_telegram_conversations()\n",
        "\n",
        "print(f\"\\nLoaded {len(telegram_conversations)} Telegram conversations\")\n",
        "\n",
        "# Filter for French conversations with more detailed logging\n",
        "print(\"\\nFiltering for French conversations...\")\n",
        "french_telegram_conversations = {}\n",
        "\n",
        "for filename, messages in telegram_conversations.items():\n",
        "    print(f\"Testing {filename} with {len(messages)} messages...\")\n",
        "    \n",
        "    # Show sample messages for debugging\n",
        "    sample_messages = messages[:5]\n",
        "    for i, msg in enumerate(sample_messages):\n",
        "        print(f\"  Sample {i+1}: {msg['message'][:80]}...\")\n",
        "    \n",
        "    is_french = is_french_conversation(messages)\n",
        "    print(f\"  French detection result: {is_french}\")\n",
        "    \n",
        "    if is_french:\n",
        "        french_telegram_conversations[filename] = messages\n",
        "        print(f\"  ✓ ADDED {filename}: {len(messages)} messages (French)\")\n",
        "    else:\n",
        "        print(f\"  ✗ SKIPPED {filename}: Not detected as French\")\n",
        "\n",
        "print(f\"\\nFound {len(french_telegram_conversations)} French Telegram conversations\")\n",
        "\n",
        "# Process French Telegram conversations\n",
        "# Note: Processing will be done later when all functions are defined\n",
        "if french_telegram_conversations:\n",
        "    print(\"\\nTelegram conversations loaded (processing will be done later)...\")\n",
        "    for filename, messages in french_telegram_conversations.items():\n",
        "        print(f\"  ✓ {filename}: {len(messages)} messages\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No French Telegram conversations found!\")\n",
        "\n",
        "# Initialize all_conversations if not already defined\n",
        "if 'all_conversations' not in globals():\n",
        "    all_conversations = {}\n",
        "\n",
        "# Add Telegram conversations to the main conversations dict\n",
        "all_conversations.update(french_telegram_conversations)\n",
        "\n",
        "print(f\"\\n=== UPDATED CONVERSATION SUMMARY ===\")\n",
        "print(f\"Total conversations (WhatsApp + French Telegram): {len(all_conversations)}\")\n",
        "total_messages = sum(len(msgs) for msgs in all_conversations.values())\n",
        "print(f\"Total messages: {total_messages:,}\")\n",
        "\n",
        "# Show breakdown by source\n",
        "whatsapp_count = len([f for f in all_conversations.keys() if not f.endswith('_telegram.txt')])\n",
        "telegram_count = len([f for f in all_conversations.keys() if f.endswith('_telegram.txt')])\n",
        "print(f\"WhatsApp conversations: {whatsapp_count}\")\n",
        "print(f\"Telegram conversations: {telegram_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import glob\n",
        "\n",
        "def parse_whatsapp_message(line: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse a single WhatsApp message line into structured data.\n",
        "    \n",
        "    Format: [DD/MM/YYYY, HH:MM:SS] Sender: Message\n",
        "    \"\"\"\n",
        "    # Regex pattern to match WhatsApp message format\n",
        "    pattern = r'\\[(\\d{2}/\\d{2}/\\d{4}), (\\d{2}:\\d{2}:\\d{2})\\] ([^:]+): (.+)'\n",
        "    match = re.match(pattern, line.strip())\n",
        "    \n",
        "    if not match:\n",
        "        return None\n",
        "    \n",
        "    date_str, time_str, sender, message = match.groups()\n",
        "    \n",
        "    # Parse datetime\n",
        "    try:\n",
        "        dt = datetime.strptime(f\"{date_str} {time_str}\", \"%d/%m/%Y %H:%M:%S\")\n",
        "    except ValueError:\n",
        "        return None\n",
        "    \n",
        "    return {\n",
        "        'timestamp': dt.isoformat(),\n",
        "        'sender': sender.strip(),\n",
        "        'message': message.strip()\n",
        "    }\n",
        "\n",
        "def load_whatsapp_conversation(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load and parse a complete WhatsApp conversation file.\n",
        "    Handles multi-line messages and system messages.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    current_message = None\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "                \n",
        "            # Check if this line starts a new message\n",
        "            parsed = parse_whatsapp_message(line)\n",
        "            \n",
        "            if parsed:\n",
        "                # Save previous message if exists\n",
        "                if current_message:\n",
        "                    messages.append(current_message)\n",
        "                \n",
        "                # Start new message\n",
        "                current_message = parsed\n",
        "            else:\n",
        "                # This is a continuation of the previous message\n",
        "                if current_message and not line.startswith('['):\n",
        "                    current_message['message'] += ' ' + line\n",
        "    \n",
        "    # Don't forget the last message\n",
        "    if current_message:\n",
        "        messages.append(current_message)\n",
        "    \n",
        "    return messages\n",
        "\n",
        "def load_all_whatsapp_conversations(data_dir: str = 'data/raw/whatsapp') -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Load all WhatsApp conversations from the data directory.\n",
        "    Returns a dictionary with filename as key and messages as value.\n",
        "    \"\"\"\n",
        "    conversations = {}\n",
        "    \n",
        "    # Find all .txt files in the WhatsApp directory\n",
        "    whatsapp_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
        "    \n",
        "    for file_path in whatsapp_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"Loading {filename}...\")\n",
        "        \n",
        "        try:\n",
        "            messages = load_whatsapp_conversation(file_path)\n",
        "            conversations[filename] = messages\n",
        "            print(f\"  Loaded {len(messages)} messages\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {filename}: {e}\")\n",
        "    \n",
        "    return conversations\n",
        "\n",
        "# Load all WhatsApp conversations\n",
        "print(\"Loading all WhatsApp conversations...\")\n",
        "all_conversations = load_all_whatsapp_conversations()\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\nLoaded {len(all_conversations)} conversations:\")\n",
        "for filename, messages in all_conversations.items():\n",
        "    print(f\"  {filename}: {len(messages)} messages\")\n",
        "\n",
        "# Show sample messages from each conversation\n",
        "print(\"\\nSample messages from each conversation:\")\n",
        "for filename, messages in all_conversations.items():\n",
        "    print(f\"\\n--- {filename} ---\")\n",
        "    for i, msg in enumerate(messages[:3]):  # Show first 3 messages\n",
        "        print(msg)\n",
        "        print(f\"{i+1}. [{msg['timestamp']}] {msg['sender']}: {msg['message'][:100]}...\")\n",
        "    if len(messages) > 3:\n",
        "        print(f\"... and {len(messages) - 3} more messages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Analysis and Statistics\n",
        "\n",
        "def analyze_conversations(conversations: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyze the loaded conversations and provide statistics.\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'total_conversations': len(conversations),\n",
        "        'total_messages': 0,\n",
        "        'senders': set(),\n",
        "        'date_range': {'start': None, 'end': None},\n",
        "        'messages_per_conversation': {},\n",
        "        'senders_per_conversation': {},\n",
        "        'platforms': set()\n",
        "    }\n",
        "    \n",
        "    all_messages = []\n",
        "    \n",
        "    for filename, messages in conversations.items():\n",
        "        stats['total_messages'] += len(messages)\n",
        "        stats['messages_per_conversation'][filename] = len(messages)\n",
        "        \n",
        "        # Collect all senders\n",
        "        senders = set(msg['sender'] for msg in messages)\n",
        "        stats['senders_per_conversation'][filename] = list(senders)\n",
        "        stats['senders'].update(senders)\n",
        "        \n",
        "        # Platform is now stored at conversation level, not message level\n",
        "        # All WhatsApp conversations have platform \"whatsapp\"\n",
        "        stats['platforms'].add('whatsapp')\n",
        "\n",
        "        # Date range\n",
        "        if messages:\n",
        "            timestamps = [msg['timestamp'] for msg in messages]\n",
        "            if stats['date_range']['start'] is None:\n",
        "                stats['date_range']['start'] = min(timestamps)\n",
        "                stats['date_range']['end'] = max(timestamps)\n",
        "            else:\n",
        "                stats['date_range']['start'] = min(stats['date_range']['start'], min(timestamps))\n",
        "                stats['date_range']['end'] = max(stats['date_range']['end'], max(timestamps))\n",
        "        \n",
        "        all_messages.extend(messages)\n",
        "    \n",
        "    # Convert sets to lists for JSON serialization\n",
        "    stats['senders'] = list(stats['senders'])\n",
        "    stats['platforms'] = list(stats['platforms'])\n",
        "    \n",
        "    return stats, all_messages\n",
        "\n",
        "# Analyze the conversations\n",
        "print(\"Analyzing conversations...\")\n",
        "stats, all_messages = analyze_conversations(all_conversations)\n",
        "\n",
        "print(f\"\\n=== CONVERSATION STATISTICS ===\")\n",
        "print(f\"Total conversations: {stats['total_conversations']}\")\n",
        "print(f\"Total messages: {stats['total_messages']}\")\n",
        "print(f\"Date range: {stats['date_range']['start']} to {stats['date_range']['end']}\")\n",
        "print(f\"Platforms: {', '.join(stats['platforms'])}\")\n",
        "print(f\"Unique senders: {len(stats['senders'])}\")\n",
        "\n",
        "print(f\"\\n=== MESSAGES PER CONVERSATION ===\")\n",
        "for filename, count in stats['messages_per_conversation'].items():\n",
        "    print(f\"  {filename}: {count:,} messages\")\n",
        "\n",
        "print(f\"\\n=== SENDERS PER CONVERSATION ===\")\n",
        "for filename, senders in stats['senders_per_conversation'].items():\n",
        "    print(f\"  {filename}: {', '.join(senders)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Message Processing Functions\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def sanitize_message(message: str) -> str:\n",
        "    \"\"\"\n",
        "    Sanitize message text by removing special characters while preserving:\n",
        "    - Emojis (Unicode emoji characters)\n",
        "    - Numbers and basic punctuation\n",
        "    - Letters and spaces\n",
        "    - Common punctuation marks (.,!?;:)\n",
        "    \n",
        "    Removes:\n",
        "    - Control characters\n",
        "    - Special symbols\n",
        "    - Non-printable characters\n",
        "    - Excessive whitespace\n",
        "    \"\"\"\n",
        "    if not message:\n",
        "        return message\n",
        "    \n",
        "    # Keep emojis, letters, numbers, spaces, and basic punctuation\n",
        "    # This regex keeps:\n",
        "    # - \\w: letters, digits, underscore\n",
        "    # - \\s: whitespace\n",
        "    # - Basic punctuation: .,!?;:()[]{}\\\"'\n",
        "    # - Unicode emoji ranges\n",
        "    # - Common currency symbols\n",
        "    # - Basic math symbols\n",
        "    \n",
        "    # Define what to keep\n",
        "    keep_pattern = r'[\\w\\s.,!?;:()\\[\\]{}\"\\'€$£¥₹+=\\-*/%<>@#&~`|\\\\]'\n",
        "    \n",
        "    # Also keep emojis (Unicode emoji ranges)\n",
        "    emoji_pattern = r'[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002600-\\U000026FF]|[\\U00002700-\\U000027BF]'\n",
        "    \n",
        "    # Combine patterns\n",
        "    sanitized = ''\n",
        "    for char in message:\n",
        "        if re.match(keep_pattern, char) or re.match(emoji_pattern, char):\n",
        "            sanitized += char\n",
        "        else:\n",
        "            # Replace with space for word separation\n",
        "            sanitized += ' '\n",
        "    \n",
        "    # Clean up multiple spaces and strip\n",
        "    sanitized = re.sub(r'\\s+', ' ', sanitized).strip()\n",
        "    \n",
        "    return sanitized\n",
        "\n",
        "def clean_username(username: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean username by removing special characters, emojis, and normalizing unicode.\n",
        "    Keeps only letters, numbers, and spaces.\n",
        "    \"\"\"\n",
        "    if not username:\n",
        "        return username\n",
        "    \n",
        "    # Normalize unicode characters (e.g., é -> e)\n",
        "    username = unicodedata.normalize('NFD', username)\n",
        "    \n",
        "    # Remove all special characters, keep only alphanumeric and spaces\n",
        "    cleaned = ''.join(c for c in username if c.isalnum() or c.isspace())\n",
        "    \n",
        "    # Clean up multiple spaces and strip\n",
        "    cleaned = ' '.join(cleaned.split())\n",
        "    \n",
        "    # If the result is empty or too short, use a fallback\n",
        "    if len(cleaned.strip()) < 1:\n",
        "        cleaned = f\"user_{hash(username) % 10000}\"\n",
        "    \n",
        "    return cleaned.strip()\n",
        "\n",
        "def sanitize_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Sanitize all message text by removing special characters while preserving emojis, numbers, and text.\n",
        "    \"\"\"\n",
        "    for msg in messages:\n",
        "        if 'message' in msg:\n",
        "            sanitized_message = sanitize_message(msg['message'])\n",
        "            msg['message'] = sanitized_message\n",
        "    \n",
        "    return messages\n",
        "\n",
        "def clean_usernames_in_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Clean usernames in all messages by removing special characters.\n",
        "    \"\"\"\n",
        "    for msg in messages:\n",
        "        if 'sender' in msg:\n",
        "            cleaned_sender = clean_username(msg['sender'])\n",
        "            msg['sender'] = cleaned_sender\n",
        "    \n",
        "    return messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified Language Detection - Per Conversation Approach\n",
        "import random\n",
        "from collections import Counter\n",
        "from langdetect import detect, DetectorFactory\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "# Set seed for consistent results\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def detect_conversation_language(messages: List[Dict[str, Any]], sample_size: int = 100) -> str:\n",
        "    \"\"\"\n",
        "    Detect the language of a conversation by sampling ~100 messages.\n",
        "    Uses only the external langdetect library for detection.\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        return 'other'\n",
        "    \n",
        "    # Sample messages for language detection (limit to ~100)\n",
        "    sample_messages = random.sample(messages, min(sample_size, len(messages)))\n",
        "    \n",
        "    # Collect text from sampled messages\n",
        "    sample_texts = []\n",
        "    for msg in sample_messages:\n",
        "        text = msg['message'].strip()\n",
        "        if len(text) >= 3:  # Only use messages with at least 3 characters\n",
        "            sample_texts.append(text)\n",
        "    \n",
        "    if not sample_texts:\n",
        "        return 'other'\n",
        "    \n",
        "    # Detect language for each sample using external library only\n",
        "    detected_languages = []\n",
        "    for text in sample_texts:\n",
        "        try:\n",
        "            detected_lang = detect(text)\n",
        "            if detected_lang in ['fr', 'en']:\n",
        "                detected_languages.append(detected_lang)\n",
        "        except LangDetectException:\n",
        "            continue\n",
        "    \n",
        "    if not detected_languages:\n",
        "        return 'other'\n",
        "    \n",
        "    # Return the most common language\n",
        "    most_common = Counter(detected_languages).most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "# Detect language for each conversation\n",
        "print(\"Detecting language for each conversation...\")\n",
        "conversation_languages = {}\n",
        "for filename, messages in all_conversations.items():\n",
        "    print(f\"Detecting language for {filename}...\")\n",
        "    language = detect_conversation_language(messages)\n",
        "    conversation_languages[filename] = language\n",
        "    print(f\"  Detected language: {language}\")\n",
        "\n",
        "# Filter messages (without individual language detection)\n",
        "print(\"\\nFiltering messages...\")\n",
        "filtered_messages = []\n",
        "\n",
        "for msg in all_messages:\n",
        "    message_text = msg['message']\n",
        "    \n",
        "    # Skip if too short\n",
        "    if len(message_text.strip()) < 3:\n",
        "        continue\n",
        "        \n",
        "    # Skip system messages\n",
        "    if any(system_msg in message_text.lower() for system_msg in [\n",
        "        'messages and calls are end-to-end encrypted',\n",
        "        'you created group',\n",
        "        'added',\n",
        "        'removed',\n",
        "        'left',\n",
        "        'omitted'\n",
        "    ]):\n",
        "        continue\n",
        "        \n",
        "    # Skip media messages\n",
        "    if any(media_msg in message_text.lower() for media_msg in [\n",
        "        'image omitted',\n",
        "        'video omitted',\n",
        "        'audio omitted',\n",
        "        'document omitted',\n",
        "        'sticker omitted'\n",
        "    ]):\n",
        "        continue\n",
        "        \n",
        "    filtered_messages.append(msg)\n",
        "\n",
        "# No need to add language to individual messages - will be stored at conversation level\n",
        "\n",
        "print(f\"Original messages: {len(all_messages):,}\")\n",
        "print(f\"Filtered messages: {len(filtered_messages):,}\")\n",
        "print(f\"Filtered out: {len(all_messages) - len(filtered_messages):,} messages\")\n",
        "\n",
        "# Language distribution by conversation\n",
        "language_counts = Counter(conversation_languages.values())\n",
        "print(f\"\\n=== LANGUAGE DISTRIBUTION BY CONVERSATION ===\")\n",
        "for lang, count in language_counts.most_common():\n",
        "    percentage = (count / len(conversation_languages)) * 100\n",
        "    print(f\"  {lang}: {count} conversations ({percentage:.1f}%)\")\n",
        "\n",
        "# Show sample messages by conversation language\n",
        "print(f\"\\n=== SAMPLE MESSAGES BY CONVERSATION LANGUAGE ===\")\n",
        "for lang in ['fr', 'en', 'other']:\n",
        "    if lang in language_counts:\n",
        "        # Find a conversation with this language\n",
        "        for filename, conv_lang in conversation_languages.items():\n",
        "            if conv_lang == lang:\n",
        "                sample_messages = [msg for msg in all_conversations[filename] if msg in filtered_messages][:3]\n",
        "                print(f\"\\n{lang.upper()} conversation ({filename}):\")\n",
        "                for i, msg in enumerate(sample_messages, 1):\n",
        "                    print(f\"  {i}. [{msg['sender']}]: {msg['message'][:80]}...\")\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing Functions\n",
        "import uuid\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def sanitize_message(message: str) -> str:\n",
        "    \"\"\"\n",
        "    Sanitize message text by removing special characters while preserving:\n",
        "    - Emojis (Unicode emoji characters)\n",
        "    - Numbers and basic punctuation\n",
        "    - Letters and spaces\n",
        "    - Common punctuation marks (.,!?;:)\n",
        "    \n",
        "    Removes:\n",
        "    - Control characters\n",
        "    - Special symbols\n",
        "    - Non-printable characters\n",
        "    - Excessive whitespace\n",
        "    \"\"\"\n",
        "    if not message:\n",
        "        return message\n",
        "    \n",
        "    # Keep emojis, letters, numbers, spaces, and basic punctuation\n",
        "    # This regex keeps:\n",
        "    # - \\w: letters, digits, underscore\n",
        "    # - \\s: whitespace\n",
        "    # - Basic punctuation: .,!?;:()[]{}\"'\n",
        "    # - Unicode emoji ranges\n",
        "    # - Common currency symbols\n",
        "    # - Basic math symbols\n",
        "    \n",
        "    # Define what to keep\n",
        "    keep_pattern = r'[\\w\\s.,!?;:()\\[\\]{}\"\\'€$£¥₹+=\\-*/%<>@#&~`|\\\\]'\n",
        "    \n",
        "    # Also keep emojis (Unicode emoji ranges)\n",
        "    emoji_pattern = r'[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002600-\\U000026FF]|[\\U00002700-\\U000027BF]'\n",
        "    \n",
        "    # Combine patterns\n",
        "    sanitized = ''\n",
        "    for char in message:\n",
        "        if re.match(keep_pattern, char) or re.match(emoji_pattern, char):\n",
        "            sanitized += char\n",
        "        else:\n",
        "            # Replace with space for word separation\n",
        "            sanitized += ' '\n",
        "    \n",
        "    # Clean up multiple spaces and strip\n",
        "    sanitized = re.sub(r'\\s+', ' ', sanitized).strip()\n",
        "    \n",
        "    return sanitized\n",
        "\n",
        "def clean_username(username: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean username by removing special characters, emojis, and normalizing unicode.\n",
        "    Keeps only letters, numbers, and spaces.\n",
        "    \"\"\"\n",
        "    if not username:\n",
        "        return username\n",
        "    \n",
        "    # Normalize unicode characters (e.g., é -> e)\n",
        "    username = unicodedata.normalize('NFD', username)\n",
        "    \n",
        "    # Remove all special characters, keep only alphanumeric and spaces\n",
        "    cleaned = ''.join(c for c in username if c.isalnum() or c.isspace())\n",
        "    \n",
        "    # Clean up multiple spaces and strip\n",
        "    cleaned = ' '.join(cleaned.split())\n",
        "    \n",
        "    # If the result is empty or too short, use a fallback\n",
        "    if len(cleaned.strip()) < 1:\n",
        "        cleaned = f\"user_{hash(username) % 10000}\"\n",
        "    \n",
        "    return cleaned.strip()\n",
        "\n",
        "def sanitize_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Sanitize all message text by removing special characters while preserving emojis, numbers, and text.\n",
        "    \"\"\"\n",
        "    for msg in messages:\n",
        "        if 'message' in msg:\n",
        "            original_message = msg['message']\n",
        "            sanitized_message = sanitize_message(original_message)\n",
        "            msg['message'] = sanitized_message\n",
        "            msg['original_message'] = original_message  # Keep original for reference\n",
        "    \n",
        "    return messages\n",
        "\n",
        "def clean_usernames_in_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Clean usernames in all messages by removing special characters.\n",
        "    \"\"\"\n",
        "    for msg in messages:\n",
        "        if 'sender' in msg:\n",
        "            original_sender = msg['sender']\n",
        "            cleaned_sender = clean_username(original_sender)\n",
        "            msg['sender'] = cleaned_sender\n",
        "            msg['original_sender'] = original_sender  # Keep original for reference\n",
        "    \n",
        "    return messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_whatsapp_messages(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse WhatsApp messages from a file and return a list of message dictionaries.\n",
        "    This is a wrapper around the existing load_whatsapp_conversation function.\n",
        "    \"\"\"\n",
        "    return load_whatsapp_conversation(file_path)\n",
        "\n",
        "def filter_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Filter messages to remove system messages, media messages, and very short messages.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    \n",
        "    for msg in messages:\n",
        "        message_text = msg['message']\n",
        "        \n",
        "        # Skip if too short\n",
        "        if len(message_text.strip()) < 3:\n",
        "            continue\n",
        "            \n",
        "        # Skip system messages\n",
        "        if any(system_msg in message_text.lower() for system_msg in [\n",
        "            'messages and calls are end-to-end encrypted',\n",
        "            'you created group',\n",
        "            'added',\n",
        "            'removed',\n",
        "            'left',\n",
        "            'omitted'\n",
        "        ]):\n",
        "            continue\n",
        "            \n",
        "        # Skip media messages\n",
        "        if any(media_msg in message_text.lower() for media_msg in [\n",
        "            'image omitted',\n",
        "            'video omitted',\n",
        "            'audio omitted',\n",
        "            'document omitted',\n",
        "            'sticker omitted'\n",
        "        ]):\n",
        "            continue\n",
        "            \n",
        "        filtered.append(msg)\n",
        "    \n",
        "    return filtered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Parse Conversations\n",
        "\n",
        "First, we'll load all WhatsApp conversations, parse the messages, and detect the language for each conversation.\n",
        "\n",
        "This step involves:\n",
        "- Loading all `.txt` files from the WhatsApp data directory\n",
        "- Parsing each file to extract structured message data\n",
        "- Sanitizing message text (removing special characters while preserving emojis, numbers, and text)\n",
        "- Cleaning usernames by removing special characters and emojis\n",
        "- Filtering out system messages, media messages, and very short messages\n",
        "- Detecting the primary language for each conversation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show message sanitization results\n",
        "print(f\"\\n=== MESSAGE SANITIZATION RESULTS ===\")\n",
        "print(\"  Messages have been sanitized to preserve emojis, numbers, and text while removing special characters\")\n",
        "\n",
        "# Show username cleaning results\n",
        "print(f\"\\n=== USERNAME CLEANING RESULTS ===\")\n",
        "unique_senders = set()\n",
        "\n",
        "for msg in all_messages:\n",
        "    if 'sender' in msg:\n",
        "        unique_senders.add(msg['sender'])\n",
        "\n",
        "print(f\"Unique senders: {len(unique_senders)}\")\n",
        "\n",
        "print(f\"\\nAll unique senders:\")\n",
        "for sender in sorted(unique_senders):\n",
        "    print(f\"  {sender}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and parse all WhatsApp conversations\n",
        "print(\"Loading WhatsApp conversations...\")\n",
        "whatsapp_files = glob.glob('data/raw/whatsapp/*.txt')\n",
        "print(f\"Found {len(whatsapp_files)} WhatsApp files: {[os.path.basename(f) for f in whatsapp_files]}\")\n",
        "\n",
        "# Initialize with existing conversations (includes Telegram)\n",
        "all_conversations = all_conversations.copy()  # Keep existing Telegram conversations\n",
        "all_messages = []\n",
        "\n",
        "# Add all existing messages to the list\n",
        "for messages in all_conversations.values():\n",
        "    all_messages.extend(messages)\n",
        "\n",
        "for file_path in whatsapp_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(f\"\\nProcessing {filename}...\")\n",
        "    \n",
        "    # Parse messages\n",
        "    messages = parse_whatsapp_messages(file_path)\n",
        "    print(f\"  Parsed {len(messages)} messages\")\n",
        "    \n",
        "    # Sanitize message text (remove special characters, keep emojis/numbers/text)\n",
        "    messages = sanitize_messages(messages)\n",
        "    print(f\"  Sanitized message text\")\n",
        "    \n",
        "    # Clean usernames (remove special characters)\n",
        "    messages = clean_usernames_in_messages(messages)\n",
        "    print(f\"  Cleaned usernames\")\n",
        "    \n",
        "    # Filter messages\n",
        "    filtered_messages = filter_messages(messages)\n",
        "    print(f\"  Filtered to {len(filtered_messages)} training messages\")\n",
        "    \n",
        "    all_conversations[filename] = filtered_messages\n",
        "    all_messages.extend(filtered_messages)\n",
        "\n",
        "print(f\"\\n=== CONVERSATION SUMMARY ===\")\n",
        "print(f\"Total conversations: {len(all_conversations)}\")\n",
        "print(f\"Total messages: {len(all_messages):,}\")\n",
        "\n",
        "# Detect language for each conversation\n",
        "print(f\"\\nDetecting conversation languages...\")\n",
        "conversation_languages = {}\n",
        "\n",
        "for filename, messages in all_conversations.items():\n",
        "    if messages:\n",
        "        language = detect_conversation_language(messages)\n",
        "        conversation_languages[filename] = language\n",
        "        print(f\"  {filename}: {language} ({len(messages)} messages)\")\n",
        "    else:\n",
        "        conversation_languages[filename] = 'other'\n",
        "        print(f\"  {filename}: no messages\")\n",
        "\n",
        "# Show language distribution\n",
        "language_counts = Counter(conversation_languages.values())\n",
        "print(f\"\\nLanguage distribution:\")\n",
        "for lang, count in language_counts.items():\n",
        "    print(f\"  {lang}: {count} conversations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Show unique senders\n",
        "print(\"Found unique senders:\")\n",
        "unique_senders = set(msg['sender'] for msg in all_messages)\n",
        "for name in sorted(unique_senders):\n",
        "    print(f\"  {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV Output Functions\n",
        "import pandas as pd\n",
        "\n",
        "def save_conversations_to_csv(conversations: Dict[str, List[Dict[str, Any]]], \n",
        "                             conversation_languages: Dict[str, str],\n",
        "                             output_dir: str = 'data/cleaned') -> None:\n",
        "    \"\"\"Save each conversation as a standardized CSV file with timestamp, sender, message columns.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    for filename, messages in conversations.items():\n",
        "        if not messages:\n",
        "            continue\n",
        "            \n",
        "        # Create DataFrame with standardized columns\n",
        "        df = pd.DataFrame(messages)\n",
        "        \n",
        "        # Ensure we have the required columns\n",
        "        required_columns = ['timestamp', 'sender', 'message']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            print(f\"Warning: Missing required columns in {filename}\")\n",
        "            continue\n",
        "        \n",
        "        # Select only the required columns and reorder\n",
        "        df_standardized = df[required_columns].copy()\n",
        "        \n",
        "        # Sort by timestamp to ensure chronological order\n",
        "        df_standardized = df_standardized.sort_values('timestamp').reset_index(drop=True)\n",
        "        \n",
        "        # Save as CSV\n",
        "        csv_filename = filename.replace('.txt', '_messages.csv')\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df_standardized.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "        \n",
        "        print(f\"Saved {len(df_standardized)} messages to {csv_path}\")\n",
        "    \n",
        "    # Note: We don't save a combined CSV as conversations should stay separate for windowing\n",
        "    \n",
        "    # Save language mapping\n",
        "    language_mapping = {\n",
        "        'conversation': list(conversation_languages.keys()),\n",
        "        'language': list(conversation_languages.values())\n",
        "    }\n",
        "    language_df = pd.DataFrame(language_mapping)\n",
        "    language_csv_path = os.path.join(output_dir, 'conversation_languages.csv')\n",
        "    language_df.to_csv(language_csv_path, index=False, encoding='utf-8')\n",
        "    print(f\"Saved language mapping to {language_csv_path}\")\n",
        "\n",
        "def create_processing_summary(conversations: Dict[str, List[Dict[str, Any]]], \n",
        "                             conversation_languages: Dict[str, str]) -> None:\n",
        "    \"\"\"Create a summary of the processing results.\"\"\"\n",
        "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
        "    print(f\"Total conversations processed: {len(conversations)}\")\n",
        "    \n",
        "    total_messages = sum(len(msgs) for msgs in conversations.values())\n",
        "    print(f\"Total messages: {total_messages:,}\")\n",
        "    \n",
        "    # Language distribution\n",
        "    language_counts = Counter(conversation_languages.values())\n",
        "    print(f\"\\nLanguage distribution:\")\n",
        "    for lang, count in language_counts.items():\n",
        "        print(f\"  {lang}: {count} conversations\")\n",
        "    \n",
        "    # Messages per conversation\n",
        "    print(f\"\\nMessages per conversation:\")\n",
        "    for filename, messages in conversations.items():\n",
        "        language = conversation_languages.get(filename, 'unknown')\n",
        "        print(f\"  {filename}: {len(messages):,} messages ({language})\")\n",
        "    \n",
        "    print(f\"\\n=== OUTPUT FILES ===\")\n",
        "    print(\"Generated CSV files in data/cleaned/:\")\n",
        "    print(\"  - *_messages.csv: Individual conversation files\")\n",
        "    print(\"  - conversation_languages.csv: Language mapping\")\n",
        "    print(\"  - Note: Conversations are kept separate for proper windowing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Step: Save all cleaned data as standardized CSV files\n",
        "\n",
        "print(\"=== SAVING CLEANED DATA AS CSV ===\")\n",
        "\n",
        "# Save conversations as standardized CSV files\n",
        "save_conversations_to_csv(\n",
        "    conversations=all_conversations,\n",
        "    conversation_languages=conversation_languages,\n",
        "    output_dir='data/cleaned'\n",
        ")\n",
        "\n",
        "# Create processing summary\n",
        "create_processing_summary(all_conversations, conversation_languages)\n",
        "\n",
        "print(\"\\n=== SAVE COMPLETE ===\")\n",
        "print(\"All cleaned data has been saved to the 'data/cleaned' directory:\")\n",
        "print(\"- Individual conversation files: *_messages.csv\")\n",
        "print(\"- Language mapping: conversation_languages.csv\")\n",
        "print(\"- Note: Conversations are kept separate for proper windowing\")\n",
        "print(\"\\nEach CSV contains standardized columns: timestamp, sender, message\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process Telegram conversations now that all functions are defined\n",
        "print(\"=== PROCESSING TELEGRAM CONVERSATIONS ===\")\n",
        "\n",
        "if 'french_telegram_conversations' in globals() and french_telegram_conversations:\n",
        "    print(\"Processing French Telegram conversations...\")\n",
        "    \n",
        "    for filename, messages in french_telegram_conversations.items():\n",
        "        print(f\"Processing {filename}...\")\n",
        "        \n",
        "        # Sanitize message text\n",
        "        messages = sanitize_messages(messages)\n",
        "        print(f\"  Sanitized message text\")\n",
        "        \n",
        "        # Clean usernames\n",
        "        messages = clean_usernames_in_messages(messages)\n",
        "        print(f\"  Cleaned usernames\")\n",
        "        \n",
        "        # Filter messages\n",
        "        filtered_messages = filter_messages(messages)\n",
        "        print(f\"  Filtered to {len(filtered_messages)} training messages\")\n",
        "        \n",
        "        # Update the conversation in all_conversations\n",
        "        all_conversations[filename] = filtered_messages\n",
        "\n",
        "print(f\"\\n=== FINAL CONVERSATION SUMMARY ===\")\n",
        "print(f\"Total conversations: {len(all_conversations)}\")\n",
        "total_messages = sum(len(msgs) for msgs in all_conversations.values())\n",
        "print(f\"Total messages: {total_messages:,}\")\n",
        "\n",
        "# Show breakdown by source\n",
        "whatsapp_count = len([f for f in all_conversations.keys() if not f.endswith('_telegram.txt')])\n",
        "telegram_count = len([f for f in all_conversations.keys() if f.endswith('_telegram.txt')])\n",
        "print(f\"WhatsApp conversations: {whatsapp_count}\")\n",
        "print(f\"Telegram conversations: {telegram_count}\")\n",
        "\n",
        "print(\"\\n✅ All conversations processed and ready for CSV export!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-export all conversations including Telegram to CSV\n",
        "print(\"=== RE-EXPORTING ALL CONVERSATIONS TO CSV ===\")\n",
        "\n",
        "# Update all_messages to include all processed conversations\n",
        "all_messages = []\n",
        "for messages in all_conversations.values():\n",
        "    all_messages.extend(messages)\n",
        "\n",
        "print(f\"Total conversations to export: {len(all_conversations)}\")\n",
        "print(f\"Total messages to export: {len(all_messages):,}\")\n",
        "\n",
        "# Detect language for all conversations (including Telegram)\n",
        "print(f\"\\nDetecting languages for all conversations...\")\n",
        "conversation_languages = {}\n",
        "\n",
        "for filename, messages in all_conversations.items():\n",
        "    if messages:\n",
        "        language = detect_conversation_language(messages)\n",
        "        conversation_languages[filename] = language\n",
        "        print(f\"  {filename}: {language} ({len(messages)} messages)\")\n",
        "    else:\n",
        "        conversation_languages[filename] = 'other'\n",
        "        print(f\"  {filename}: no messages\")\n",
        "\n",
        "# Save all conversations as CSV files\n",
        "print(f\"\\nSaving all conversations to CSV...\")\n",
        "save_conversations_to_csv(\n",
        "    conversations=all_conversations,\n",
        "    conversation_languages=conversation_languages,\n",
        "    output_dir='data/cleaned'\n",
        ")\n",
        "\n",
        "# Final summary\n",
        "create_processing_summary(all_conversations, conversation_languages)\n",
        "\n",
        "print(\"\\n🎉 ALL CONVERSATIONS (WhatsApp + Telegram) EXPORTED SUCCESSFULLY!\")\n",
        "print(\"Check the 'data/cleaned' directory for all CSV files.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
