{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Prepare Messages for Training\n",
        "\n",
        "This notebook handles loading CSV messages and preparing them for model fine-tuning.\n",
        "\n",
        "## Overview\n",
        "- **Load standardized CSV messages** from notebook 01\n",
        "- **Normalize speakers** using user configuration (user → A:, others → B:)\n",
        "- **Merge consecutive messages** from same speaker (< 3 min apart)\n",
        "- **Segment conversations** (≥ 30 min gaps)\n",
        "- **Generate rolling windows** for training with proper B: → A: patterns\n",
        "- **Format as JSONL** for fine-tuning following PREPARE.md specifications\n",
        "- **Split into train/validation sets** chronologically\n",
        "\n",
        "## Input Data\n",
        "- **CSV files** with columns: `timestamp`, `sender`, `message`\n",
        "- From `data/cleaned/` folder (generated by notebook 01)\n",
        "- **User configuration** from `config/user_config.json`\n",
        "\n",
        "## Output Data\n",
        "- **Training dataset**: `data/processed/train.jsonl`\n",
        "- **Validation dataset**: `data/processed/val.jsonl`\n",
        "- **Dataset statistics** and metrics\n",
        "- **PREPARE.md compliant** training samples\n",
        "\n",
        "## Key Features\n",
        "- **User-configurable** speaker identification\n",
        "- **PREPARE.md compliant** windowing (context ends with B:, target is A:)\n",
        "- **Chronological data split** (90% train, 10% validation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def load_user_config(config_path: str = 'config/user_config.json') -> Dict[str, Any]:\n",
        "    \"\"\"Load user configuration for identifying the user's messages.\"\"\"\n",
        "    if not os.path.exists(config_path):\n",
        "        # Create default config if it doesn't exist\n",
        "        default_config = {\n",
        "            \"user_identifiers\": [\"Antonin\", \"antonin\", \"Anto\", \"anto\"],\n",
        "            \"user_label\": \"A:\",\n",
        "            \"other_label\": \"B:\",\n",
        "            \"description\": \"Configuration for identifying the user's messages in chat data.\"\n",
        "        }\n",
        "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "        with open(config_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(default_config, f, indent=2)\n",
        "        print(f\"Created default user config at {config_path}\")\n",
        "        return default_config\n",
        "    \n",
        "    with open(config_path, 'r', encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded user config: {config['user_identifiers']} -> {config['user_label']}\")\n",
        "    return config\n",
        "\n",
        "def load_csv_messages(data_dir: str = 'data/cleaned') -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, str]]:\n",
        "    \"\"\"Load CSV messages from the cleaned data directory, keeping conversations separate.\"\"\"\n",
        "    conversations = {}\n",
        "    conversation_languages = {}\n",
        "    \n",
        "    # Load language mapping\n",
        "    language_file = os.path.join(data_dir, 'conversation_languages.csv')\n",
        "    if os.path.exists(language_file):\n",
        "        lang_df = pd.read_csv(language_file)\n",
        "        conversation_languages = dict(zip(lang_df['conversation'], lang_df['language']))\n",
        "    \n",
        "    # Load individual conversation CSV files\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith('_messages.csv'):\n",
        "            file_path = os.path.join(data_dir, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            \n",
        "            # Convert to list of dictionaries\n",
        "            messages = df.to_dict('records')\n",
        "            \n",
        "            # Add conversation name and language\n",
        "            conv_name = filename.replace('_messages.csv', '')\n",
        "            for msg in messages:\n",
        "                msg['conversation'] = conv_name\n",
        "                msg['language'] = conversation_languages.get(conv_name, 'unknown')\n",
        "            \n",
        "            # Sort messages within this conversation by timestamp\n",
        "            messages.sort(key=lambda x: x['timestamp'])\n",
        "            conversations[conv_name] = messages\n",
        "    \n",
        "    return conversations, conversation_languages\n",
        "\n",
        "def normalize_speakers(messages: List[Dict[str, Any]], user_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Normalize speaker names using user configuration.\"\"\"\n",
        "    normalized_messages = []\n",
        "    user_identifiers = user_config['user_identifiers']\n",
        "    user_label = user_config['user_label']\n",
        "    other_label = user_config['other_label']\n",
        "    \n",
        "    for msg in messages:\n",
        "        normalized_msg = msg.copy()\n",
        "        sender = msg['sender'].strip()\n",
        "        \n",
        "        # Check if sender matches any user identifier\n",
        "        is_user = any(identifier.lower() in sender.lower() for identifier in user_identifiers)\n",
        "        \n",
        "        if is_user:\n",
        "            normalized_msg['sender'] = user_label\n",
        "        else:\n",
        "            normalized_msg['sender'] = other_label\n",
        "        \n",
        "        normalized_messages.append(normalized_msg)\n",
        "    \n",
        "    return normalized_messages\n",
        "\n",
        "def merge_consecutive_messages(messages: List[Dict[str, Any]], max_gap_minutes: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Merge consecutive messages from the same speaker if they are less than max_gap_minutes apart.\"\"\"\n",
        "    if not messages:\n",
        "        return messages\n",
        "    \n",
        "    merged_messages = []\n",
        "    current_message = None\n",
        "    \n",
        "    for msg in messages:\n",
        "        if current_message is None:\n",
        "            current_message = msg.copy()\n",
        "            # Handle NaN values in message field\n",
        "            if pd.isna(current_message['message']):\n",
        "                current_message['message'] = ''\n",
        "            continue\n",
        "        \n",
        "        # Check if same speaker and within time gap\n",
        "        same_speaker = current_message['sender'] == msg['sender']\n",
        "        within_gap = False\n",
        "        \n",
        "        try:\n",
        "            current_time = datetime.fromisoformat(current_message['timestamp'].replace('Z', '+00:00'))\n",
        "            msg_time = datetime.fromisoformat(msg['timestamp'].replace('Z', '+00:00'))\n",
        "            time_diff = msg_time - current_time\n",
        "            within_gap = time_diff.total_seconds() < (max_gap_minutes * 60)\n",
        "        except:\n",
        "            within_gap = False\n",
        "        \n",
        "        if same_speaker and within_gap:\n",
        "            # Merge messages - handle NaN values\n",
        "            current_msg_text = current_message['message'] if not pd.isna(current_message['message']) else ''\n",
        "            msg_text = msg['message'] if not pd.isna(msg['message']) else ''\n",
        "            \n",
        "            if current_msg_text and msg_text:\n",
        "                current_message['message'] = current_msg_text + ' ' + msg_text\n",
        "            elif msg_text:  # Only add if msg has content\n",
        "                current_message['message'] = msg_text\n",
        "            # Keep the latest timestamp\n",
        "            current_message['timestamp'] = msg['timestamp']\n",
        "        else:\n",
        "            # Save current message and start new one\n",
        "            merged_messages.append(current_message)\n",
        "            current_message = msg.copy()\n",
        "            # Handle NaN values in message field\n",
        "            if pd.isna(current_message['message']):\n",
        "                current_message['message'] = ''\n",
        "    \n",
        "    # Don't forget the last message\n",
        "    if current_message:\n",
        "        merged_messages.append(current_message)\n",
        "    \n",
        "    return merged_messages\n",
        "\n",
        "def segment_conversations(messages: List[Dict[str, Any]], gap_minutes: int = 30) -> List[List[Dict[str, Any]]]:\n",
        "    \"\"\"Segment conversations based on time gaps. Start new segment if gap >= gap_minutes.\"\"\"\n",
        "    if not messages:\n",
        "        return []\n",
        "    \n",
        "    segments = []\n",
        "    current_segment = [messages[0]]\n",
        "    \n",
        "    for i in range(1, len(messages)):\n",
        "        try:\n",
        "            prev_time = datetime.fromisoformat(messages[i-1]['timestamp'].replace('Z', '+00:00'))\n",
        "            curr_time = datetime.fromisoformat(messages[i]['timestamp'].replace('Z', '+00:00'))\n",
        "            time_diff = curr_time - prev_time\n",
        "            \n",
        "            if time_diff.total_seconds() >= (gap_minutes * 60):\n",
        "                # Start new segment\n",
        "                segments.append(current_segment)\n",
        "                current_segment = [messages[i]]\n",
        "            else:\n",
        "                current_segment.append(messages[i])\n",
        "        except:\n",
        "            # If timestamp parsing fails, continue in same segment\n",
        "            current_segment.append(messages[i])\n",
        "    \n",
        "    # Add the last segment\n",
        "    if current_segment:\n",
        "        segments.append(current_segment)\n",
        "    \n",
        "    return segments\n",
        "\n",
        "def process_single_conversation(messages: List[Dict[str, Any]], user_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Process a single conversation: normalize speakers, merge consecutive, segment.\"\"\"\n",
        "    # Normalize speakers\n",
        "    normalized = normalize_speakers(messages, user_config)\n",
        "    \n",
        "    # Merge consecutive messages\n",
        "    merged = merge_consecutive_messages(normalized, max_gap_minutes=3)\n",
        "    \n",
        "    # Segment by time gaps\n",
        "    segments = segment_conversations(merged, gap_minutes=30)\n",
        "    \n",
        "    return segments\n",
        "\n",
        "def generate_rolling_windows(segments: List[List[Dict[str, Any]]], \n",
        "                           window_sizes: List[int] = [6, 8, 10, 12], \n",
        "                           stride: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Generate rolling windows from conversation segments.\"\"\"\n",
        "    windows = []\n",
        "    \n",
        "    for segment in segments:\n",
        "        if len(segment) < 2:  # Need at least 2 messages for a window\n",
        "            continue\n",
        "        \n",
        "        # Try different window sizes\n",
        "        for window_size in window_sizes:\n",
        "            for i in range(0, len(segment) - window_size, stride):\n",
        "                window = segment[i:i + window_size]\n",
        "                \n",
        "                # Check if window ends with B: and next message is A:\n",
        "                if (i + window_size < len(segment) and \n",
        "                    window[-1]['sender'] == 'B:' and \n",
        "                    segment[i + window_size]['sender'] == 'A:'):\n",
        "                    \n",
        "                    # Create window data\n",
        "                    window_data = {\n",
        "                        'window': window,\n",
        "                        'next_message': segment[i + window_size],\n",
        "                        'window_size': window_size,\n",
        "                        'segment_id': id(segment),\n",
        "                        'conversation': window[0]['conversation']  # Add conversation ID\n",
        "                    }\n",
        "                    windows.append(window_data)\n",
        "    \n",
        "    return windows\n",
        "\n",
        "def format_window_as_jsonl(window_data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Format a window as JSONL training sample.\"\"\"\n",
        "    window = window_data['window']\n",
        "    next_message = window_data['next_message']\n",
        "    \n",
        "    # Build chat text\n",
        "    chat_lines = []\n",
        "    for msg in window:\n",
        "        chat_lines.append(f\"{msg['sender']} {msg['message']}\")\n",
        "    \n",
        "    chat_text = '\\n'.join(chat_lines)\n",
        "    \n",
        "    # Create training sample for base model completion format\n",
        "    sample = {\n",
        "        \"text\": f\"{chat_text}\\nA: {next_message['message']}</s>\"\n",
        "    }\n",
        "    \n",
        "    return json.dumps(sample, ensure_ascii=False)\n",
        "\n",
        "def create_instruct_seeds(messages: List[Dict[str, Any]], user_config: Dict[str, Any], sample_rate: float = 0.1) -> List[str]:\n",
        "    \"\"\"Create instruct seed samples from user's replies.\"\"\"\n",
        "    user_label = user_config['user_label']\n",
        "    other_label = user_config['other_label']\n",
        "    user_messages = [msg for msg in messages if msg['sender'] == user_label]\n",
        "    \n",
        "    # Sample 10% of user's messages\n",
        "    num_samples = max(1, int(len(user_messages) * sample_rate))\n",
        "    sampled_messages = random.sample(user_messages, min(num_samples, len(user_messages)))\n",
        "    \n",
        "    seeds = []\n",
        "    for msg in sampled_messages:\n",
        "        # Find the previous other speaker message for context\n",
        "        msg_index = messages.index(msg)\n",
        "        prev_other_message = None\n",
        "        \n",
        "        for i in range(msg_index - 1, -1, -1):\n",
        "            if messages[i]['sender'] == other_label:\n",
        "                prev_other_message = messages[i]\n",
        "                break\n",
        "        \n",
        "        if prev_other_message:\n",
        "            seed_text = f\"<sys>Write a realistic text chat. Keep it short.</sys>\\n<seed>{other_label} {prev_other_message['message']}</seed>\\n{user_label}\"\n",
        "            sample = {\"text\": seed_text}\n",
        "            seeds.append(json.dumps(sample, ensure_ascii=False))\n",
        "    \n",
        "    return seeds\n",
        "\n",
        "def split_dataset(messages: List[Dict[str, Any]], train_ratio: float = 0.9) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
        "    \"\"\"Split dataset chronologically: 90% training, 10% validation.\"\"\"\n",
        "    split_index = int(len(messages) * train_ratio)\n",
        "    train_messages = messages[:split_index]\n",
        "    val_messages = messages[split_index:]\n",
        "    \n",
        "    return train_messages, val_messages\n",
        "\n",
        "def save_jsonl_dataset(samples: List[str], filename: str) -> None:\n",
        "    \"\"\"Save samples to JSONL file.\"\"\"\n",
        "    os.makedirs('data/processed', exist_ok=True)\n",
        "    filepath = os.path.join('data/processed', filename)\n",
        "    \n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        for sample in samples:\n",
        "            f.write(sample + '\\n')\n",
        "    \n",
        "    print(f\"Saved {len(samples)} samples to {filepath}\")\n",
        "\n",
        "def calculate_dataset_stats(windows: List[Dict[str, Any]], train_samples: List[str], val_samples: List[str]) -> None:\n",
        "    \"\"\"Calculate and print dataset statistics.\"\"\"\n",
        "    print(f\"\\n=== DATASET STATISTICS ===\")\n",
        "    print(f\"Number of segments: {len(set(w['segment_id'] for w in windows))}\")\n",
        "    print(f\"Number of training samples: {len(train_samples)}\")\n",
        "    print(f\"Number of validation samples: {len(val_samples)}\")\n",
        "    \n",
        "    # Calculate median context length\n",
        "    context_lengths = []\n",
        "    for window_data in windows:\n",
        "        window = window_data['window']\n",
        "        total_length = sum(len(msg['message']) for msg in window)\n",
        "        context_lengths.append(total_length)\n",
        "    \n",
        "    if context_lengths:\n",
        "        context_lengths.sort()\n",
        "        median_length = context_lengths[len(context_lengths) // 2]\n",
        "        print(f\"Median context length: {median_length} characters\")\n",
        "    \n",
        "    print(f\"Total samples: {len(train_samples) + len(val_samples)}\")\n",
        "    print(f\"Train/Val ratio: {len(train_samples)}/{len(val_samples)} ({len(train_samples)/(len(train_samples)+len(val_samples))*100:.1f}%/{len(val_samples)/(len(train_samples)+len(val_samples))*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load CSV Messages\n",
        "\n",
        "Load the standardized CSV messages from notebook 01 and prepare them for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CSV messages from notebook 01 - keeping conversations separate\n",
        "print(\"Loading CSV messages from notebook 01...\")\n",
        "conversations, conversation_languages = load_csv_messages()\n",
        "\n",
        "print(f\"\\n=== LOADED DATA SUMMARY ===\")\n",
        "total_messages = sum(len(messages) for messages in conversations.values())\n",
        "print(f\"Total conversations: {len(conversations)}\")\n",
        "print(f\"Total messages: {total_messages:,}\")\n",
        "print(f\"Languages detected: {dict(Counter(conversation_languages.values()))}\")\n",
        "\n",
        "# Show conversation breakdown\n",
        "print(f\"\\n=== CONVERSATION BREAKDOWN ===\")\n",
        "for conv_name, messages in conversations.items():\n",
        "    language = conversation_languages.get(conv_name, 'unknown')\n",
        "    print(f\"  {conv_name}: {len(messages):,} messages ({language})\")\n",
        "\n",
        "# Show sample of loaded data\n",
        "print(f\"\\n=== SAMPLE MESSAGES ===\")\n",
        "first_conv = next(iter(conversations.values()))\n",
        "for i, msg in enumerate(first_conv[:5]):\n",
        "    print(f\"{i+1}. [{msg['sender']}]: {msg['message'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Normalize Speakers and Process Messages\n",
        "\n",
        "Normalize speaker names, merge consecutive messages, segment conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load user configuration\n",
        "print(\"Loading user configuration...\")\n",
        "user_config = load_user_config()\n",
        "\n",
        "# Process each conversation separately\n",
        "print(\"\\nProcessing conversations separately...\")\n",
        "all_segments = []\n",
        "total_original = 0\n",
        "total_processed = 0\n",
        "\n",
        "for conv_name, messages in conversations.items():\n",
        "    print(f\"\\nProcessing {conv_name}...\")\n",
        "    \n",
        "    # Process this conversation\n",
        "    segments = process_single_conversation(messages, user_config)\n",
        "    \n",
        "    # Flatten segments to get message count\n",
        "    processed_messages = [msg for segment in segments for msg in segment]\n",
        "    \n",
        "    print(f\"  {conv_name}: {len(messages)} → {len(processed_messages)} messages, {len(segments)} segments\")\n",
        "    \n",
        "    total_original += len(messages)\n",
        "    total_processed += len(processed_messages)\n",
        "    all_segments.extend(segments)\n",
        "\n",
        "print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
        "print(f\"Total messages: {total_original} → {total_processed}\")\n",
        "print(f\"Total segments: {len(all_segments)}\")\n",
        "\n",
        "# Show segment statistics\n",
        "segment_lengths = [len(seg) for seg in all_segments]\n",
        "if segment_lengths:\n",
        "    print(f\"Segment lengths: min={min(segment_lengths)}, max={max(segment_lengths)}, avg={sum(segment_lengths)/len(segment_lengths):.1f}\")\n",
        "\n",
        "# Show sample of processed messages from first segment\n",
        "print(f\"\\n=== SAMPLE PROCESSED MESSAGES ===\")\n",
        "if all_segments:\n",
        "    first_segment = all_segments[0]\n",
        "    print(f\"Sample from conversation: {first_segment[0]['conversation']}\")\n",
        "    for i, msg in enumerate(first_segment[:5]):\n",
        "        print(f\"{i+1}. [{msg['sender']}]: {msg['message'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate Training Windows\n",
        "\n",
        "Generate rolling windows and create training samples in JSONL format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate rolling windows from segments\n",
        "print(\"Generating rolling windows...\")\n",
        "windows = generate_rolling_windows(all_segments, window_sizes=[6, 8, 10, 12], stride=3)\n",
        "print(f\"✓ Generated {len(windows)} training windows\")\n",
        "\n",
        "# Show window statistics\n",
        "window_sizes = [w['window_size'] for w in windows]\n",
        "window_size_counts = Counter(window_sizes)\n",
        "print(f\"Window size distribution: {dict(window_size_counts)}\")\n",
        "\n",
        "# Show conversation distribution in windows\n",
        "conv_counts = Counter(w['conversation'] for w in windows)\n",
        "print(f\"Windows per conversation: {dict(conv_counts)}\")\n",
        "\n",
        "# Format windows as JSONL training samples\n",
        "print(\"\\nFormatting training samples...\")\n",
        "training_samples = []\n",
        "for window_data in windows:\n",
        "    sample = format_window_as_jsonl(window_data)\n",
        "    training_samples.append(sample)\n",
        "\n",
        "print(f\"✓ Created {len(training_samples)} training samples\")\n",
        "\n",
        "# Create instruct seed samples (10% of user's replies)\n",
        "print(\"\\nCreating instruct seed samples...\")\n",
        "# Flatten all processed messages for instruct seeds\n",
        "all_processed_messages = [msg for segment in all_segments for msg in segment]\n",
        "instruct_seeds = create_instruct_seeds(all_processed_messages, user_config, sample_rate=0.1)\n",
        "print(f\"✓ Created {len(instruct_seeds)} instruct seed samples\")\n",
        "\n",
        "# Combine regular training samples with instruct seeds\n",
        "all_training_samples = training_samples + instruct_seeds\n",
        "print(f\"✓ Total training samples: {len(all_training_samples)}\")\n",
        "\n",
        "# Show sample training data\n",
        "print(f\"\\n=== SAMPLE TRAINING DATA ===\")\n",
        "print(\"Regular training sample:\")\n",
        "print(training_samples[0] if training_samples else \"No samples\")\n",
        "print(\"\\nInstruct seed sample:\")\n",
        "print(instruct_seeds[0] if instruct_seeds else \"No seeds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Split Dataset and Save\n",
        "\n",
        "Split dataset chronologically and save training/validation files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split windows by conversation (90% train, 10% val)\n",
        "print(\"\\nSplitting windows by conversation...\")\n",
        "\n",
        "# Split windows maintaining conversation boundaries\n",
        "train_windows = []\n",
        "val_windows = []\n",
        "\n",
        "# Group windows by conversation\n",
        "conv_windows = {}\n",
        "for window in windows:\n",
        "    conv_name = window['conversation']\n",
        "    if conv_name not in conv_windows:\n",
        "        conv_windows[conv_name] = []\n",
        "    conv_windows[conv_name].append(window)\n",
        "\n",
        "# Split each conversation's windows\n",
        "for conv_name, conv_window_list in conv_windows.items():\n",
        "    split_index = int(len(conv_window_list) * 0.9)\n",
        "    train_windows.extend(conv_window_list[:split_index])\n",
        "    val_windows.extend(conv_window_list[split_index:])\n",
        "\n",
        "# Generate training samples\n",
        "print(\"\\nFormatting samples...\")\n",
        "train_samples = [format_window_as_jsonl(w) for w in train_windows]\n",
        "val_samples = [format_window_as_jsonl(w) for w in val_windows]\n",
        "\n",
        "# Create instruct seeds from processed messages\n",
        "all_processed_messages = [msg for segment in all_segments for msg in segment]\n",
        "all_instruct_seeds = create_instruct_seeds(all_processed_messages, user_config, sample_rate=0.1)\n",
        "\n",
        "# Split instruct seeds similarly\n",
        "train_instruct_count = int(len(all_instruct_seeds) * 0.9)\n",
        "train_instruct_seeds = all_instruct_seeds[:train_instruct_count]\n",
        "val_instruct_seeds = all_instruct_seeds[train_instruct_count:]\n",
        "\n",
        "# Combine samples\n",
        "train_all_samples = train_samples + train_instruct_seeds\n",
        "val_all_samples = val_samples + val_instruct_seeds\n",
        "\n",
        "print(f\"✓ Training samples: {len(train_all_samples):,} ({len(train_samples):,} windows + {len(train_instruct_seeds):,} seeds)\")\n",
        "print(f\"✓ Validation samples: {len(val_all_samples):,} ({len(val_samples):,} windows + {len(val_instruct_seeds):,} seeds)\")\n",
        "\n",
        "# Show conversation distribution in splits\n",
        "train_conv_counts = Counter(w['conversation'] for w in train_windows)\n",
        "val_conv_counts = Counter(w['conversation'] for w in val_windows)\n",
        "print(f\"Train conversations: {dict(train_conv_counts)}\")\n",
        "print(f\"Val conversations: {dict(val_conv_counts)}\")\n",
        "\n",
        "# Save datasets\n",
        "print(\"\\nSaving datasets...\")\n",
        "save_jsonl_dataset(train_all_samples, 'train.jsonl')\n",
        "save_jsonl_dataset(val_all_samples, 'val.jsonl')\n",
        "\n",
        "# Calculate and display statistics\n",
        "calculate_dataset_stats(windows, train_all_samples, val_all_samples)\n",
        "\n",
        "print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
        "total_messages = sum(len(messages) for messages in conversations.values())\n",
        "print(f\"✓ Processed {total_messages:,} total messages from {len(conversations)} conversations\")\n",
        "print(f\"✓ Maintained conversation boundaries throughout processing\")\n",
        "print(f\"✓ Generated rolling windows per conversation\")\n",
        "print(f\"✓ Split dataset maintaining conversation integrity\")\n",
        "print(f\"✓ Saved train.jsonl and val.jsonl with proper target format\")\n",
        "print(f\"✓ Ready for model fine-tuning\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
